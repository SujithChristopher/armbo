{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from ultralytics.utils.plotting import Annotator\n",
    "from ultralytics.utils.benchmarks import benchmark\n",
    "import yaml\n",
    "import ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.150  Python-3.11.4 torch-2.0.1+cu118 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "Setup complete  (24 CPUs, 15.7 GB RAM, 134.9/280.0 GB disk)\n"
     ]
    }
   ],
   "source": [
    "# check if YOLO\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = YOLO('yolov8n-pose.pt')\n",
    "# model = YOLO('yolov8n-pose.pt').load(r\"D:\\CMC\\DeepVision\\training_notebooks\\runs\\pose\\train2\\weights\\last.pt\")\n",
    "model = YOLO(r'D:\\CMC\\DeepVision\\training_notebooks\\runs\\pose\\train3\\weights\\last.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDict = {\"path\":\"../dataset/multi_class/dataset_processed/dataset.yaml\",\n",
    "          \"train\":\"../dataset/multi_class/dataset_processed/images/train\",\n",
    "          \"val\":\"../dataset/multi_class/dataset_processed/images/val\",\n",
    "          \"test\":\"../dataset/multi_class/dataset_processed/images/test\",\n",
    "          \"kpt_shape\":[5, 2],\n",
    "          'names': {0: '0',\n",
    "                    1: \"49\",\n",
    "                    2: \"80\"}\n",
    "          }\n",
    "\n",
    "# with open(\"../dataset/multi_class/dataset_processed/dataset.yaml\", 'w') as file:\n",
    "#     documents = yaml.dump(myDict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.150  Python-3.11.4 torch-2.0.1+cu118 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=pose, mode=train, model=D:\\CMC\\DeepVision\\training_notebooks\\runs\\pose\\train3\\weights\\last.pt, data=D:\\CMC\\DeepVision\\dataset\\multi_class\\dataset.yaml, epochs=60, patience=50, batch=48, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=0, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=True, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=True, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=True, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\pose\\train3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    824023  ultralytics.nn.modules.head.Pose             [3, [5, 2], [64, 128, 256]]   \n",
      "YOLOv8n-pose summary: 250 layers, 3083559 parameters, 3083543 gradients\n",
      "\n",
      "Transferred 397/397 items from pretrained weights\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\CMC\\DeepVision\\dataset\\multi_class\\dataset_processed\\labels\\train.cache... 61904 images, 0 backgrounds, 0 corrupt: 100%|██████████| 61904/61904 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\CMC\\DeepVision\\dataset\\multi_class\\dataset_processed\\labels\\val.cache... 7261 images, 0 backgrounds, 0 corrupt: 100%|██████████| 7261/7261 [00:00<?, ?it/s]\n",
      "Plotting labels to runs\\pose\\train3\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 63 weight(decay=0.0), 73 weight(decay=0.000375), 72 bias(decay=0.0)\n",
      "Resuming training from D:\\CMC\\DeepVision\\training_notebooks\\runs\\pose\\train3\\weights\\last.pt from epoch 43 to 60 total epochs\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\pose\\train3\u001b[0m\n",
      "Starting training for 60 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      43/60         7G     0.2117    0.02318          0     0.1364     0.7722        127        640: 100%|██████████| 1290/1290 [31:23<00:00,  1.46s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 76/76 [03:21<00:00,  2.65s/it]\n",
      "                   all       7261      16081      0.999      0.999      0.995      0.991      0.999      0.999      0.995      0.995\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      44/60      7.01G     0.2108     0.0237          0     0.1358     0.7718        137        640: 100%|██████████| 1290/1290 [32:07<00:00,  1.49s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 76/76 [02:41<00:00,  2.12s/it]\n",
      "                   all       7261      16081      0.999      0.999      0.995      0.991      0.999      0.999      0.995      0.995\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      45/60         7G     0.2093    0.02382          0     0.1349     0.7724        119        640: 100%|██████████| 1290/1290 [31:24<00:00,  1.46s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 76/76 [02:44<00:00,  2.17s/it]\n",
      "                   all       7261      16081      0.999      0.999      0.995      0.991      0.999      0.999      0.995      0.995\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      46/60      7.01G      0.206    0.02279          0     0.1325     0.7713        122        640: 100%|██████████| 1290/1290 [31:31<00:00,  1.47s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 76/76 [02:43<00:00,  2.15s/it]\n",
      "                   all       7261      16081      0.999      0.999      0.995      0.991      0.999      0.999      0.995      0.995\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      47/60      7.03G     0.2046    0.02281          0     0.1316     0.7718        130        640: 100%|██████████| 1290/1290 [35:25<00:00,  1.65s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 76/76 [08:30<00:00,  6.72s/it]\n",
      "                   all       7261      16081      0.999      0.999      0.995      0.992      0.999      0.999      0.995      0.995\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      48/60      7.01G     0.2017    0.02166          0     0.1298      0.772        142        640: 100%|██████████| 1290/1290 [39:17<00:00,  1.83s/it] \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 76/76 [02:45<00:00,  2.18s/it]\n",
      "                   all       7261      16081      0.999      0.999      0.995      0.992      0.999      0.999      0.995      0.995\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      49/60         7G     0.1996    0.02187          0     0.1279     0.7716        130        640: 100%|██████████| 1290/1290 [32:42<00:00,  1.52s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 76/76 [02:46<00:00,  2.19s/it]\n",
      "                   all       7261      16081      0.999      0.999      0.995      0.992      0.999      0.999      0.995      0.995\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      50/60      7.01G     0.1969     0.0208          0     0.1264     0.7712        142        640: 100%|██████████| 1290/1290 [32:48<00:00,  1.53s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 76/76 [02:46<00:00,  2.20s/it]\n",
      "                   all       7261      16081      0.999      0.999      0.995      0.992      0.999      0.999      0.995      0.995\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      51/60      6.91G     0.1813    0.01416          0     0.1205      0.763         74        640: 100%|██████████| 1290/1290 [31:12<00:00,  1.45s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 76/76 [02:41<00:00,  2.12s/it]\n",
      "                   all       7261      16081      0.999      0.999      0.995      0.992      0.999      0.999      0.995      0.995\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      52/60      6.91G     0.1756    0.01336          0     0.1168      0.763         59        640: 100%|██████████| 1290/1290 [31:12<00:00,  1.45s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 76/76 [02:45<00:00,  2.17s/it]\n",
      "                   all       7261      16081      0.999      0.999      0.995      0.992      0.999      0.999      0.995      0.995\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      53/60      6.91G     0.1709    0.01256          0     0.1139     0.7618         71        640: 100%|██████████| 1290/1290 [31:04<00:00,  1.45s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 76/76 [02:55<00:00,  2.30s/it]\n",
      "                   all       7261      16081      0.999      0.999      0.995      0.992      0.999      0.999      0.995      0.995\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      54/60      6.91G     0.1678    0.01219          0     0.1118     0.7616         69        640: 100%|██████████| 1290/1290 [30:50<00:00,  1.43s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 76/76 [02:51<00:00,  2.26s/it]\n",
      "                   all       7261      16081      0.999      0.999      0.995      0.992      0.999      0.999      0.995      0.995\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      55/60      6.91G     0.1646    0.01167          0     0.1094     0.7614         72        640: 100%|██████████| 1290/1290 [31:02<00:00,  1.44s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 76/76 [02:57<00:00,  2.33s/it]\n",
      "                   all       7261      16081      0.999      0.999      0.995      0.992      0.999      0.999      0.995      0.995\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      56/60      6.91G     0.1612    0.01132          0     0.1068     0.7609         69        640: 100%|██████████| 1290/1290 [30:59<00:00,  1.44s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 76/76 [02:47<00:00,  2.20s/it]\n",
      "                   all       7261      16081      0.999          1      0.995      0.992      0.999          1      0.995      0.995\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      57/60      6.91G     0.1578    0.01085          0     0.1039     0.7604         78        640: 100%|██████████| 1290/1290 [31:09<00:00,  1.45s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 76/76 [02:40<00:00,  2.11s/it]\n",
      "                   all       7261      16081      0.999          1      0.995      0.992      0.999          1      0.995      0.995\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      58/60      6.91G     0.1542    0.01058          0     0.1013     0.7595         67        640: 100%|██████████| 1290/1290 [31:19<00:00,  1.46s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 76/76 [02:40<00:00,  2.12s/it]\n",
      "                   all       7261      16081          1          1      0.995      0.992          1          1      0.995      0.995\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      59/60      6.91G     0.1509    0.01011          0    0.09858     0.7597         69        640: 100%|██████████| 1290/1290 [31:11<00:00,  1.45s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 76/76 [02:41<00:00,  2.13s/it]\n",
      "                   all       7261      16081          1          1      0.995      0.992          1          1      0.995      0.995\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      60/60      6.91G     0.1477   0.009715          0    0.09611     0.7596         67        640: 100%|██████████| 1290/1290 [31:09<00:00,  1.45s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 76/76 [02:50<00:00,  2.24s/it]\n",
      "                   all       7261      16081          1          1      0.995      0.992          1          1      0.995      0.995\n",
      "\n",
      "18 epochs completed in 10.571 hours.\n",
      "Optimizer stripped from runs\\pose\\train3\\weights\\last.pt, 6.4MB\n",
      "Optimizer stripped from runs\\pose\\train3\\weights\\best.pt, 6.4MB\n",
      "\n",
      "Validating runs\\pose\\train3\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.0.150  Python-3.11.4 torch-2.0.1+cu118 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "YOLOv8n-pose summary (fused): 187 layers, 3078263 parameters, 0 gradients\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):   0%|          | 0/76 [00:00<?, ?it/s]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):   1%|▏         | 1/76 [00:02<02:44,  2.19s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):   3%|▎         | 2/76 [00:04<02:39,  2.15s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):   4%|▍         | 3/76 [00:06<02:42,  2.22s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):   5%|▌         | 4/76 [00:08<02:34,  2.15s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):   7%|▋         | 5/76 [00:10<02:24,  2.03s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):   8%|▊         | 6/76 [00:12<02:20,  2.01s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):   9%|▉         | 7/76 [00:14<02:14,  1.95s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  11%|█         | 8/76 [00:16<02:10,  1.91s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  12%|█▏        | 9/76 [00:17<02:06,  1.89s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  13%|█▎        | 10/76 [00:19<02:05,  1.90s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  14%|█▍        | 11/76 [00:21<02:01,  1.86s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  16%|█▌        | 12/76 [00:23<01:58,  1.85s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  17%|█▋        | 13/76 [00:25<01:56,  1.85s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  18%|█▊        | 14/76 [00:27<01:53,  1.83s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  20%|█▉        | 15/76 [00:28<01:52,  1.84s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  21%|██        | 16/76 [00:30<01:49,  1.82s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  22%|██▏       | 17/76 [00:32<01:47,  1.82s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  24%|██▎       | 18/76 [00:34<01:44,  1.81s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  25%|██▌       | 19/76 [00:36<01:42,  1.79s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  26%|██▋       | 20/76 [00:37<01:39,  1.78s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  28%|██▊       | 21/76 [00:39<01:39,  1.80s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  29%|██▉       | 22/76 [00:41<01:36,  1.78s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  30%|███       | 23/76 [00:43<01:35,  1.80s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  32%|███▏      | 24/76 [00:45<01:35,  1.84s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  33%|███▎      | 25/76 [00:47<01:34,  1.86s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  34%|███▍      | 26/76 [00:48<01:30,  1.81s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  36%|███▌      | 27/76 [00:50<01:28,  1.80s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  37%|███▋      | 28/76 [00:52<01:25,  1.79s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  38%|███▊      | 29/76 [00:54<01:23,  1.78s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  39%|███▉      | 30/76 [00:55<01:22,  1.78s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  41%|████      | 31/76 [00:57<01:20,  1.79s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  42%|████▏     | 32/76 [00:59<01:18,  1.79s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  43%|████▎     | 33/76 [01:01<01:16,  1.77s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  45%|████▍     | 34/76 [01:03<01:15,  1.81s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  46%|████▌     | 35/76 [01:04<01:14,  1.82s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  47%|████▋     | 36/76 [01:06<01:13,  1.83s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  49%|████▊     | 37/76 [01:08<01:10,  1.81s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  50%|█████     | 38/76 [01:10<01:09,  1.82s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  51%|█████▏    | 39/76 [01:12<01:07,  1.82s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  53%|█████▎    | 40/76 [01:14<01:05,  1.81s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  54%|█████▍    | 41/76 [01:15<01:03,  1.80s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  55%|█████▌    | 42/76 [01:17<01:02,  1.83s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  57%|█████▋    | 43/76 [01:19<01:00,  1.84s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  58%|█████▊    | 44/76 [01:21<00:58,  1.82s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  59%|█████▉    | 45/76 [01:23<00:55,  1.80s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  61%|██████    | 46/76 [01:24<00:54,  1.80s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  62%|██████▏   | 47/76 [01:26<00:53,  1.84s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  63%|██████▎   | 48/76 [01:28<00:52,  1.86s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  64%|██████▍   | 49/76 [01:30<00:50,  1.88s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  66%|██████▌   | 50/76 [01:32<00:49,  1.92s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  67%|██████▋   | 51/76 [01:34<00:47,  1.89s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  68%|██████▊   | 52/76 [01:36<00:45,  1.88s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  70%|██████▉   | 53/76 [01:38<00:42,  1.86s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  71%|███████   | 54/76 [01:40<00:41,  1.90s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  72%|███████▏  | 55/76 [01:42<00:40,  1.91s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  74%|███████▎  | 56/76 [01:43<00:37,  1.88s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  75%|███████▌  | 57/76 [01:45<00:35,  1.86s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  76%|███████▋  | 58/76 [01:47<00:33,  1.87s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  78%|███████▊  | 59/76 [01:49<00:31,  1.86s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  79%|███████▉  | 60/76 [01:51<00:29,  1.85s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  80%|████████  | 61/76 [01:53<00:27,  1.86s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  82%|████████▏ | 62/76 [01:55<00:26,  1.86s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  83%|████████▎ | 63/76 [01:57<00:24,  1.89s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  84%|████████▍ | 64/76 [01:58<00:22,  1.87s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  86%|████████▌ | 65/76 [02:00<00:20,  1.90s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  87%|████████▋ | 66/76 [02:02<00:18,  1.88s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  88%|████████▊ | 67/76 [02:04<00:17,  1.90s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  89%|████████▉ | 68/76 [02:06<00:15,  1.90s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  91%|█████████ | 69/76 [02:08<00:13,  1.94s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  92%|█████████▏| 70/76 [02:10<00:11,  1.95s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  93%|█████████▎| 71/76 [02:12<00:09,  1.92s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  95%|█████████▍| 72/76 [02:14<00:07,  1.88s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  96%|█████████▌| 73/76 [02:16<00:05,  1.89s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  97%|█████████▋| 74/76 [02:18<00:03,  1.92s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):  99%|█████████▊| 75/76 [02:19<00:01,  1.91s/it]WARNING  PoseModel has not supported augment inference yet! Now using single-scale inference instead.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 76/76 [02:21<00:00,  1.86s/it]\n",
      "                   all       7261      16081          1          1      0.995      0.992          1          1      0.995      0.995\n",
      "                    12       7261       4703          1          1      0.995      0.992          1          1      0.995      0.995\n",
      "                    88       7261       6183      0.999      0.999      0.995      0.992      0.999      0.999      0.995      0.995\n",
      "                    80       7261       5195          1          1      0.995      0.992          1          1      0.995      0.995\n",
      "Speed: 0.1ms preprocess, 1.2ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\pose\\train3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# model.train(data=r'D:\\CMC\\DeepVision\\dataset\\multi_class\\dataset.yaml', epochs=60, plots=True, workers=0, batch=48, int8=True, half=True ,\n",
    "#             augment = True)\n",
    "model.train(resume=True, workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = YOLO(r\"C:\\Users\\CMC\\Documents\\openposelibs\\pose\\DeepVision\\models\\kaggle_aruco_multiclass.pt\")\n",
    "# model = YOLO(r\"C:\\Users\\CMC\\Documents\\openposelibs\\pose\\DeepVision\\models\\kaggle_aruco_multiclass.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 D:\\CMC\\DeepVision\\dataset\\multi_class\\dataset_processed\\images\\test\\image_4368.png: 384x640 1 12, 1 88, 1 80, 121.4ms\n",
      "Speed: 12.0ms preprocess, 121.4ms inference, 30.3ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "result = model.predict(r\"D:\\CMC\\DeepVision\\dataset\\multi_class\\dataset_processed\\images\\test\\image_4368.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\Pintu\\miniconda3\\envs\\py11\\Lib\\site-packages\\ultralytics\\assets\\bus.jpg: 640x480 (no detections), 152.3ms\n",
      "Speed: 4.2ms preprocess, 152.3ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Ultralytics YOLOv8.0.147  Python-3.11.4 torch-2.0.1+cu118 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\CMC\\DeepVision\\dataset\\multi_class\\dataset_processed\\labels\\val.cache... 1190 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1190/1190 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 1190/1190 [00:53<00:00, 22.43it/s]\n",
      "                   all       1190       2644      0.996      0.999      0.995      0.994      0.996      0.999      0.995      0.995\n",
      "Speed: 1.4ms preprocess, 11.8ms inference, 0.0ms loss, 2.0ms postprocess per image\n",
      "Ultralytics YOLOv8.0.147  Python-3.11.4 torch-2.0.1+cu118 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'D:\\CMC\\DeepVision\\training_notebooks\\runs\\pose\\train2\\weights\\last.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 17, 8400) (6.1 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m starting export with torch 2.0.1+cu118...\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m export success  1.1s, saved as 'D:\\CMC\\DeepVision\\training_notebooks\\runs\\pose\\train2\\weights\\last.torchscript' (12.2 MB)\n",
      "\n",
      "Export complete (1.3s)\n",
      "Results saved to \u001b[1mD:\\CMC\\DeepVision\\training_notebooks\\runs\\pose\\train2\\weights\u001b[0m\n",
      "Predict:         yolo predict task=pose model=D:\\CMC\\DeepVision\\training_notebooks\\runs\\pose\\train2\\weights\\last.torchscript imgsz=640 \n",
      "Validate:        yolo val task=pose model=D:\\CMC\\DeepVision\\training_notebooks\\runs\\pose\\train2\\weights\\last.torchscript imgsz=640 data=None \n",
      "Visualize:       https://netron.app\n",
      "Loading D:\\CMC\\DeepVision\\training_notebooks\\runs\\pose\\train2\\weights\\last.torchscript for TorchScript inference...\n",
      "\n",
      "image 1/1 C:\\Users\\Pintu\\miniconda3\\envs\\py11\\Lib\\site-packages\\ultralytics\\assets\\bus.jpg: 640x640 (no detections), 62.0ms\n",
      "Speed: 3.8ms preprocess, 62.0ms inference, 5.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Ultralytics YOLOv8.0.147  Python-3.11.4 torch-2.0.1+cu118 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "Loading D:\\CMC\\DeepVision\\training_notebooks\\runs\\pose\\train2\\weights\\last.torchscript for TorchScript inference...\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\CMC\\DeepVision\\dataset\\multi_class\\dataset_processed\\labels\\val.cache... 1190 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1190/1190 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 1190/1190 [00:42<00:00, 27.84it/s]\n",
      "                   all       1190       2644      0.996      0.999      0.995      0.994      0.996      0.999      0.995      0.995\n",
      "Speed: 1.1ms preprocess, 9.3ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
      "Ultralytics YOLOv8.0.147  Python-3.11.4 torch-2.0.1+cu118 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'D:\\CMC\\DeepVision\\training_notebooks\\runs\\pose\\train2\\weights\\last.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 17, 8400) (6.1 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.14.0 opset 17...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  4.0s, saved as 'D:\\CMC\\DeepVision\\training_notebooks\\runs\\pose\\train2\\weights\\last.onnx' (12.1 MB)\n",
      "\n",
      "Export complete (4.0s)\n",
      "Results saved to \u001b[1mD:\\CMC\\DeepVision\\training_notebooks\\runs\\pose\\train2\\weights\u001b[0m\n",
      "Predict:         yolo predict task=pose model=D:\\CMC\\DeepVision\\training_notebooks\\runs\\pose\\train2\\weights\\last.onnx imgsz=640 \n",
      "Validate:        yolo val task=pose model=D:\\CMC\\DeepVision\\training_notebooks\\runs\\pose\\train2\\weights\\last.onnx imgsz=640 data=None \n",
      "Visualize:       https://netron.app\n",
      "Loading D:\\CMC\\DeepVision\\training_notebooks\\runs\\pose\\train2\\weights\\last.onnx for ONNX Runtime inference...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['onnxruntime'] not found, attempting AutoUpdate...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting onnxruntime\n",
      "  Obtaining dependency information for onnxruntime from https://files.pythonhosted.org/packages/01/ba/58f52bf735687c9dedf662f59ea7e110972481cd4101145c4de528abce35/onnxruntime-1.15.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading onnxruntime-1.15.1-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting coloredlogs (from onnxruntime)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "     -------------------------------------- 46.0/46.0 kB 567.9 kB/s eta 0:00:00\n",
      "Collecting flatbuffers (from onnxruntime)\n",
      "  Obtaining dependency information for flatbuffers from https://files.pythonhosted.org/packages/6f/12/d5c79ee252793ffe845d58a913197bfa02ae9a0b5c9bc3dc4b58d477b9e7/flatbuffers-23.5.26-py2.py3-none-any.whl.metadata\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Requirement already satisfied: numpy>=1.24.2 in c:\\users\\pintu\\miniconda3\\envs\\py11\\lib\\site-packages (from onnxruntime) (1.25.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\pintu\\miniconda3\\envs\\py11\\lib\\site-packages (from onnxruntime) (23.1)\n",
      "Requirement already satisfied: protobuf in c:\\users\\pintu\\miniconda3\\envs\\py11\\lib\\site-packages (from onnxruntime) (4.23.4)\n",
      "Requirement already satisfied: sympy in c:\\users\\pintu\\miniconda3\\envs\\py11\\lib\\site-packages (from onnxruntime) (1.11.1)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "     -------------------------------------- 86.8/86.8 kB 814.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\pintu\\miniconda3\\envs\\py11\\lib\\site-packages (from sympy->onnxruntime) (1.2.1)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime)\n",
      "  Downloading pyreadline3-3.4.1-py3-none-any.whl (95 kB)\n",
      "     -------------------------------------- 95.2/95.2 kB 903.9 kB/s eta 0:00:00\n",
      "Downloading onnxruntime-1.15.1-cp311-cp311-win_amd64.whl (6.7 MB)\n",
      "   ---------------------------------------- 6.7/6.7 MB 967.9 kB/s eta 0:00:00\n",
      "Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Installing collected packages: pyreadline3, flatbuffers, humanfriendly, coloredlogs, onnxruntime\n",
      "Successfully installed coloredlogs-15.0.1 flatbuffers-23.5.26 humanfriendly-10.0 onnxruntime-1.15.1 pyreadline3-3.4.1\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success  34.5s, installed 1 package: ['onnxruntime']\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m  \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "\n",
      "image 1/1 C:\\Users\\Pintu\\miniconda3\\envs\\py11\\Lib\\site-packages\\ultralytics\\assets\\bus.jpg: 640x640 (no detections), 37.7ms\n",
      "Speed: 4.0ms preprocess, 37.7ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Ultralytics YOLOv8.0.147  Python-3.11.4 torch-2.0.1+cu118 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "Loading D:\\CMC\\DeepVision\\training_notebooks\\runs\\pose\\train2\\weights\\last.onnx for ONNX Runtime inference...\n",
      "Forcing batch=1 square inference (1,3,640,640) for non-PyTorch models\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\CMC\\DeepVision\\dataset\\multi_class\\dataset_processed\\labels\\val.cache... 1190 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1190/1190 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 1190/1190 [02:04<00:00,  9.56it/s]\n",
      "                   all       1190       2644      0.996      0.999      0.995      0.994      0.996      0.999      0.995      0.995\n",
      "Speed: 2.2ms preprocess, 67.9ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
      "ERROR  Benchmark failure for OpenVINO: inference not supported on GPU\n",
      "Ultralytics YOLOv8.0.147  Python-3.11.4 torch-2.0.1+cu118 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'D:\\CMC\\DeepVision\\training_notebooks\\runs\\pose\\train2\\weights\\last.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 17, 8400) (6.1 MB)\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m export failure  0.0s: No module named 'tensorrt'\n",
      "ERROR  Benchmark failure for TensorRT: No module named 'tensorrt'\n",
      "ERROR  Benchmark failure for CoreML: inference not supported on GPU\n",
      "Ultralytics YOLOv8.0.147  Python-3.11.4 torch-2.0.1+cu118 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'D:\\CMC\\DeepVision\\training_notebooks\\runs\\pose\\train2\\weights\\last.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 17, 8400) (6.1 MB)\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['tensorflow'] not found, attempting AutoUpdate...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m  Command 'pip install --no-cache \"tensorflow\"  ' returned non-zero exit status 1.\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m export failure  645.6s: No module named 'tensorflow'\n",
      "ERROR  Benchmark failure for TensorFlow SavedModel: No module named 'tensorflow'\n",
      "Ultralytics YOLOv8.0.147  Python-3.11.4 torch-2.0.1+cu118 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'D:\\CMC\\DeepVision\\training_notebooks\\runs\\pose\\train2\\weights\\last.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 17, 8400) (6.1 MB)\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['tensorflow'] not found, attempting AutoUpdate...\n"
     ]
    }
   ],
   "source": [
    "benchmark(model=r'D:\\CMC\\DeepVision\\training_notebooks\\runs\\pose\\train2\\weights\\last.pt', data=r'D:\\CMC\\DeepVision\\dataset\\multi_class\\dataset.yaml', imgsz=640, half=False, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
